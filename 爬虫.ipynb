{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9fbe388-241c-4e5b-a2dc-9af98577ba3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.edge.service import Service\n",
    "def start_web():\n",
    "    #登陆该网站\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.common.by import By\n",
    "    \n",
    "    # 启动浏览器\n",
    "    webdriver_path = 'msedgedriver_123.exe'\n",
    "    service = Service(executable_path=webdriver_path)\n",
    "    driver = webdriver.Edge(service = service)\n",
    "    url = 'https://policy.ckcest.cn/login'\n",
    "    t = driver.get('https://policy.ckcest.cn/login')\n",
    "    \n",
    "    #隐式等待10s\n",
    "    driver.implicitly_wait(10)\n",
    "    \n",
    "    \n",
    "    # 获取账号、密码和验证码图片的元素\n",
    "    username_input = driver.find_element(By.ID,'username')\n",
    "    password_input = driver.find_element(By.ID,'password')\n",
    "    captcha_img = driver.find_element(By.ID,'imgVcode')\n",
    "    text_input = driver.find_element(By.ID,'imageverifycode')\n",
    "    \n",
    "     \n",
    "    # 获取验证码图片URL\n",
    "    captcha_img_url = captcha_img.get_attribute('src')\n",
    "    #print('验证码图片URL:', captcha_img_url)\n",
    "    \n",
    "    import base64\n",
    "    \n",
    "    # 从Base64编码的数据中提取图片内容\n",
    "    base64_data = captcha_img_url[23:]\n",
    "    #print(base64_data)\n",
    "    image_data = base64.b64decode(base64_data)\n",
    "    \n",
    "    # 将图片内容保存为图片文件\n",
    "    with open('captcha_image.jpg', 'wb') as f:\n",
    "        f.write(image_data)\n",
    "    \n",
    "    #print('验证码图片已保存为captcha_image.jpg文件')\n",
    "    \n",
    "    #识别验证码\n",
    "    \n",
    "    def get_file_content(filePath):\n",
    "          with open(filePath, \"rb\") as fp:\n",
    "             return fp.read()\n",
    "    \n",
    "    image = get_file_content('captcha_image.jpg')\n",
    "    #百度 0M0xY1SZ2evc6fh13rS0Z2WE  7lpqROeY6AqKAbArx8j5uLZ8i3owYobe\n",
    "    \n",
    "    from aip import AipOcr\n",
    "    \n",
    "    #你的 APPID AK SK \n",
    "    APP_ID = '58266535'\n",
    "    API_KEY = '0M0xY1SZ2evc6fh13rS0Z2WE'\n",
    "    SECRET_KEY = '7lpqROeY6AqKAbArx8j5uLZ8i3owYobe'\n",
    "    \n",
    "    client = AipOcr(APP_ID, API_KEY, SECRET_KEY)\n",
    "    res_image = client.basicGeneral(image)\n",
    "    text = res_image['words_result'][0]['words']\n",
    "    print(text)\n",
    "    \n",
    "    # 输入账号、密码和验证码\n",
    "    username_input.send_keys('qweasdzxcrtyfghvbn')\n",
    "    password_input.send_keys('?5201314Dyf')\n",
    "    text_input.send_keys(text)\n",
    "    \n",
    "    #点击登陆按钮，有时会出现没有成功识别的情况\n",
    "    submit_button = driver.find_element(By.ID,'submitid').click()\n",
    "    \n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8473def6-9a3e-4c08-aa0a-c61fa24be8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#爬取单个页面\n",
    "import csv\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "\n",
    "#验证页面链接是否对应的是该文件\n",
    "def open_passage_by_row(start, end):\n",
    "    \n",
    "    # 读取xlsx文件\n",
    "    xlsx_path = '地方汇总-29190条.xlsx'\n",
    "    df = pd.read_excel(xlsx_path)\n",
    "    driver = start_web()\n",
    "    # 遍历每一行\n",
    "    for index, row in df.iterrows():\n",
    "        # 获取标题和URL字段的值\n",
    "        id = row['地方编号']\n",
    "        title = row['标题']\n",
    "        url = row['标题链接']\n",
    "        if int(id) >= start and int(id) <= end:\n",
    "            # 打印或处理标题和URL\n",
    "            print(f\"ID:{id}, 标题: {title}, URL: {url}\")\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                #获取文章标题等字段写入文件\n",
    "                open_title = driver.find_element(By.XPATH,'//*[@id=\"gridleft\"]/div/div[1]/h2')\n",
    "                get_important_element(id, title, url, driver)\n",
    "                result = paqukuai(driver)\n",
    "                xieru_csv(id, title, result)\n",
    "                print(\"------------------------------------------------------------------\")\n",
    "            except TimeoutException or WebDriverException:\n",
    "                open_passage_by_row(int(id), end)\n",
    "    print(\"已完成爬取！\")\n",
    "                \n",
    "    \n",
    "#判断网页中存在的块\n",
    "def paqukuai(driver):\n",
    "    #初始化字典\n",
    "    yinyong_dict = {}\n",
    "    beiyinyong_dict = {}\n",
    "    xiangsi_dict = {}\n",
    "    \n",
    "    #对法宝联想下面的块进行爬取\n",
    "    try:\n",
    "        # 获得相关信息有多少个\n",
    "        parent_element = driver.find_element(By.XPATH, '//*[@id=\"pkulaw-association\"]')        \n",
    "        # 在这个parent_element中查找所有div标签\n",
    "        inside_element = parent_element.find_elements(By.CLASS_NAME,'lenovo')        \n",
    "        # 计算找到的div标签数量\n",
    "        number_of_divs = len(inside_element)\n",
    "        \n",
    "        for i in range(number_of_divs):\n",
    "            div_element = inside_element[i]\n",
    "            #print(div_element)\n",
    "            head_of_div = div_element.find_element(By.TAG_NAME, 'h4').text\n",
    "            #print(str(head_of_div)[:4])\n",
    "            if str(head_of_div)[:4] == \"本篇引用\":\n",
    "                print(\"本篇引用的法规:\")\n",
    "                yinyong_dict = BenPianYinYong(div_element)\n",
    "                print(yinyong_dict)\n",
    "            if str(head_of_div)[:4] == \"引用本篇\":\n",
    "                print(\"引用本篇的法规 案例 论文:\")\n",
    "                beiyinyong_dict = BenPianYinYong(div_element)\n",
    "                print(beiyinyong_dict)\n",
    "                \n",
    "    except NoSuchElementException:\n",
    "        print(\"没有法宝联想\")\n",
    "    \n",
    "    #对智能发现下面的块进行爬取\n",
    "    try:\n",
    "        # 获得相关信息有多少个\n",
    "        parent_element = driver.find_element(By.XPATH, '//*[@id=\"rightZhfx\"]')         \n",
    "        # 在这个parent_element中查找所有div标签\n",
    "        inside_element = parent_element.find_elements(By.CLASS_NAME,'lenovo')        \n",
    "        # 计算找到的div标签数量\n",
    "        number_of_divs = len(inside_element)        \n",
    "        #print(\"找到的智能发现div标签数量:\", number_of_divs)        \n",
    "        for i in range(number_of_divs):\n",
    "            div_element = inside_element[i]\n",
    "            head_of_div = div_element.find_element(By.TAG_NAME, 'h4').text\n",
    "            if head_of_div == \"本篇相似法规发现\":\n",
    "                xiangsi_dict = BenPianYinYong(div_element)\n",
    "                print(\"本篇相似法规发现:\")\n",
    "                print(xiangsi_dict)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        print(\"没有智能发现\")\n",
    "        \n",
    "    return yinyong_dict, beiyinyong_dict, xiangsi_dict\n",
    "\n",
    "#判断有无“更多”，有则点击进入，无则不进入。或者不爬更多也行 \n",
    "\n",
    "\n",
    "#爬取块并存入字典\n",
    "def BenPianYinYong(div_element):\n",
    "    # 初始化一个空字典来存储法规名称和链接\n",
    "    laws_dict = {}   \n",
    "    # 使用Selenium查找所有的<a>标签\n",
    "    a_tags = div_element.find_elements(By.TAG_NAME,'a')    \n",
    "    for a_tag in a_tags:\n",
    "        # 获取法规名称\n",
    "        law_name = a_tag.text       \n",
    "        # 获取法规对应的链接\n",
    "        law_link = a_tag.get_attribute('href')            \n",
    "        # 存入字典\n",
    "        laws_dict[law_name] = law_link   \n",
    "    #print(laws_dict)\n",
    "    return laws_dict\n",
    "\n",
    "#写入引用的csv文件\n",
    "def xieru_csv(id, title, result):\n",
    "    yinyong_dict = result[0]\n",
    "    beiyinyong_dict = result[1]\n",
    "    xiangsi_dict = result[2]\n",
    "    rows = []\n",
    "    # 准备要写入的数据\n",
    "    headers = ['政策A_id', '政策A_标题', '关系', '政策B_标题', '政策B的链接']\n",
    "    if len(yinyong_dict) != 0:\n",
    "        for key, value in yinyong_dict.items():\n",
    "            relation = (id, title, '引用', key, value)\n",
    "            rows.append(relation)\n",
    "    if len(beiyinyong_dict) != 0:\n",
    "        for key, value in beiyinyong_dict.items():\n",
    "            relation = (id, title, '被引用', key, value)\n",
    "            rows.append(relation)\n",
    "    if len(xiangsi_dict) != 0:\n",
    "        for key, value in xiangsi_dict.items():\n",
    "            relation = (id, title, '相似于', key, value)\n",
    "            rows.append(relation)\n",
    "    \n",
    "    # 写入CSV文件\n",
    "    with open('policies_local_25000.csv', 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)       \n",
    "        # 写入标题行\n",
    "        #writer.writerow(headers)        \n",
    "        # 写入数据行\n",
    "        for row in rows:\n",
    "            writer.writerow(row)\n",
    "\n",
    "#获取重要字段\n",
    "def get_important_element(id, title, url, driver):\n",
    "    def get_office(i):\n",
    "        #制定机关outer_xpath\n",
    "        office = []\n",
    "        office_outer_element = i.find_element(By.XPATH, '..')   \n",
    "        office_inner_elements = office_outer_element.find_elements(By.TAG_NAME, 'a')   \n",
    "        for i in office_inner_elements:\n",
    "            office_name = i.text\n",
    "            if office_name != '' and office_name != '机构沿革' :\n",
    "                office.append(office_name)\n",
    "        return office\n",
    "    \n",
    "    #法宝引证码\n",
    "    fabao_id = driver.find_element(By.XPATH, '//*[@id=\"gridleft\"]/div/div[1]/div[1]/span/a').text\n",
    "        \n",
    "    #获取发文字号、发文日期、施行日期等一系列元素\n",
    "    document_num = ''\n",
    "    post_data = ''\n",
    "    effective_data = ''\n",
    "    category = ''\n",
    "    timeliness = ''\n",
    "    rank_of_service = ''\n",
    "    office = []\n",
    "    \n",
    "    outer_element = driver.find_element(By.XPATH, '//*[@id=\"gridleft\"]/div/div[1]/div[2]/ul')\n",
    "    inner_elements = outer_element.find_elements(By.TAG_NAME, 'strong') \n",
    "    for i in inner_elements:\n",
    "        parent_div_text = i.find_element(By.XPATH, \"..\").text\n",
    "        if i.text == '发文字号：':\n",
    "            document_num =  parent_div_text[6:]\n",
    "        elif i.text == '公布日期：':\n",
    "            post_data = parent_div_text[6:]\n",
    "        elif i.text == '施行日期：':\n",
    "            effective_data = parent_div_text[6:]\n",
    "        elif i.text == '法规类别：':\n",
    "            category = parent_div_text[6:].split(' ')\n",
    "        elif i.text == '时效性：':\n",
    "            timeliness = parent_div_text[5:]\n",
    "        elif i.text == '效力位阶：':\n",
    "            rank_of_service = parent_div_text[6:]\n",
    "        elif i.text == '制定机关：':\n",
    "            office = get_office(i)\n",
    "\n",
    "    row = (id, title, url, fabao_id, office, document_num, post_data, effective_data, timeliness, rank_of_service, category)\n",
    "\n",
    "    with open('full_element_25000.csv', 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)             \n",
    "        writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb56f91-f4d2-4a8d-97b0-6d840f5be95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_passage_by_row(24500,25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3a4dbb-2948-428e-86c8-3d75ab781165",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
